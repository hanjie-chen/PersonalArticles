`temperature` 和 `top_p` 都是 LLM 生成文本时的采样控制参数，它们影响模型的随机性和输出的多样性。下面是详细介绍：

### 1. Temperature（温度）

- 作用：控制输出的随机性。
- 范围：通常在 `0.0` 到 `2.0` 之间（一般 `0.7` 是一个较常用的默认值）。
- 工作原理：
  - 低 `temperature`（接近 `0`）：模型更倾向于选择最高概率的词，使输出更确定、更有逻辑性，但可能缺乏创意。
  - 高 `temperature`（接近 `2`）：模型会更随机地选择词汇，可能生成更有创造性的内容，但也可能变得无意义或不连贯。

使用建议：

- 当你希望生成确定性更高、专业性更强的回答（如技术解释、数学推理）时，使用较低的 temperature（如 `0.2 - 0.5`）。
- 当你希望生成更加开放、创造性更强的文本（如故事、诗歌），可以尝试较高的 temperature（如 `0.8 - 1.5`）。

------

### 2. Top-p（核采样）

- 作用：控制模型在每次选择下一个词时考虑的概率分布范围。
- 范围：`0.0 - 1.0`（默认值通常是 `0.9`）。
- 工作原理：
  - `top_p = 1.0`：模型在采样时会考虑所有可能的单词（即无约束的采样）。
  - `top_p = 0.9`：模型会仅从累积概率达到 `90%` 的最可能的单词中选择，忽略剩下的低概率词汇。
  - `top_p = 0.5`：仅考虑最高的 `50%` 概率的单词，使模型更确定、更受控。

使用建议：

- 较低的 `top_p`（如 `0.5`）：适用于需要更严格控制输出质量的场景（如总结、解释）。
- 较高的 `top_p`（如 `0.9 - 1.0`）：适用于需要多样性的场景（如写作、对话）。

------

### 如何组合使用？

- 你通常不需要同时调整 `temperature` 和 `top_p`，可以固定一个，调整另一个：
  - 如果 `temperature` 高，则降低 `top_p`（比如 `temperature=1.0, top_p=0.7`）——更随机但仍有限制。
  - 如果 `temperature` 低，则提高 `top_p`（比如 `temperature=0.3, top_p=0.9`）——更确定但不太死板。

示例推荐：

| 应用场景       | Temperature | Top-p     |
| -------------- | ----------- | --------- |
| 严谨的技术回答 | 0.2 - 0.5   | 0.9 - 1.0 |
| 文章/博客写作  | 0.7 - 1.0   | 0.8 - 1.0 |
| 诗歌/创意写作  | 1.0 - 1.5   | 0.8 - 1.0 |
| 代码生成       | 0.2 - 0.4   | 0.9 - 1.0 |

如果你只是想让回答更稳定，建议先降低 `temperature`（如 `0.3 - 0.5`）；如果你希望输出更有创造性，可以增加 `temperature` 并降低 `top_p`（如 `1.0, 0.7`）。